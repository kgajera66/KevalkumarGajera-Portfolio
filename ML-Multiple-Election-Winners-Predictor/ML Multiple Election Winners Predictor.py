# -*- coding: utf-8 -*-
"""KY_method_Train_Model_7_Candidate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7z89pygkpwdC7TxHibDTph1-HQarwol
"""

#Import Library and packages
import pandas as pd #To manipulate and analysis Dataframe
import numpy as np #To compute numerical (array)
import math #Provides mathematical functions and constants
from numpy.lib.npyio import savez_compressed #To save NumPy arrays in a compressed format
from itertools import permutations as perm #To generate all possible permutations of a given iterable.
from itertools import combinations #To generate all possible combinations of a given iterable.
from tqdm import tqdm #To display progress bars
from google.colab import drive #To import file from drive
drive.mount('/content/drive')

#inputs as excel file

data_raw = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Voting_DataSet_08_Candidate.xlsx",sheet_name='Dataset with Remaing Pref')
NUM_candidates = 7
NUM_voters_col = "Number of voters"

import warnings
warnings.filterwarnings("ignore")

# Preprocess the input excel file and manage the data records
def preprocess_and_imputation(data_raw):

  df_raw = data_raw.copy()
  df_raw = df_raw.drop(columns=['Unnamed: 9'])

  for ind,row in df_raw.iterrows():
  # print(ind)
    if type(row['Remaining Preferene'])==type("d"):
      #print(type(row['Remaining Preferene']),row['Remaining Preferene'])
      #print('y')
      impute_val = row['Remaining Preferene'].split(",")
    # print(impute_val)
      i = 0
      for col in df_raw.columns:
        #print(df_new.loc[ind,col])
        if col !="Remaining Preferene" and math.isnan(df_raw.loc[ind,col]):
          #print("y")
          df_raw.loc[ind,col] = int(impute_val[i])
          i = i+1

  df_raw = df_raw.drop(columns=['Remaining Preferene'])
  df_raw = df_raw.astype('int')
  df_raw.rename(columns={NUM_voters_col:"Number of voters"}, inplace=True)
  df_raw = df_raw.drop_duplicates()

  return df_raw

#transform 8 candidate data into 7 candidate data
def make_7_cand_data(df_preprocessed):

  df_return = df_preprocessed.copy()

  for ind,row in df_return.iterrows():
    if row['Preference 8'] !=8:

      last_num = row['Preference 8']
      for col in df_return.columns:
        if col not in ['Number of voters','Preference 8']:
          if row[col] > last_num:
            df_return.loc[ind,col] = df_return.loc[ind,col] -1

  df_return = df_return.drop(columns=['Preference 8'])

  df_return = df_return.groupby(by=['Preference 1', 'Preference 2', 'Preference 3',
       'Preference 4', 'Preference 5', 'Preference 6', 'Preference 7'],as_index=False)['Number of voters'].sum()

  df_return = df_return.sample(frac=1)
  df_return = df_return.drop_duplicates()
  df_return.reset_index(drop=True,inplace=True)
  return df_return

#Pivot the dataframe to make useful for kemeny young method
def preference_to_candidate(preference_df):

  req = pd.DataFrame(columns=["Preference "+str(num) for num in range(1,NUM_candidates+1,1)])

  pre = list(preference_df.columns)
  pre.remove(NUM_voters_col)
  for ind,row in preference_df.iterrows():
    df_dict = {}
    for col in pre:
      prefer = int(col.split(" ")[1])

      cand = "Preference"+" " + str(row[col])
      df_dict[cand] = prefer

    #req = req.append(df_dict,ignore_index=True)
    req = pd.concat([req, pd.DataFrame.from_records([df_dict])]).reset_index(drop=True)

  req[NUM_voters_col] = preference_df[NUM_voters_col]

  return req

#Kemeny young method calculation to get the target
def ky_method(df_q,voters_col="Number of voters"):

  #if voters_col not in list(df.columns):
   # df = df.rename(columns={'No Of Voters':voters_col})

  df = df_q.copy()
  df['vote_percentage'] = df[voters_col]/(df[voters_col].sum())
  cand = list(df.columns)
  cand.remove(voters_col)
  cand.remove('vote_percentage')
  cand_orders = []
  scores = []
  for comb in perm(cand):
    comb_score = 0
    for pair in combinations(comb,2):
      temp_score = df[df[pair[0]] < df[pair[1]]]['vote_percentage'].sum()
      comb_score = comb_score + temp_score
    cand_orders.append(comb)
    scores.append(comb_score)

  df_return = pd.DataFrame()
  df_return['Preference'] = cand_orders
  df_return['Score'] = scores
  ret = df_return.sort_values(by=['Score'],ascending=False).reset_index().iloc[0,1]
  return ret

#Generate chunck of 5 rows of full data
def generate_chunks(df,num_row_combined):

  df_lst = []

  if df.shape[0] % num_row_combined != 0:
    remain = df.shape[0] % num_row_combined
    df = df.iloc[:df.shape[0]-remain,:]

  for ind in range(0,df.shape[0],num_row_combined):
    lower = ind
    higher = ind + num_row_combined

    df_chunk = df.iloc[lower:higher,:]

    df_lst.append(df_chunk)

  return df_lst

#generate target for every chunks
def gen_target_and_orders(df_lst):

  labels_lst, orders_lst = [], []
  for df in tqdm(df_lst):
  # borda_dict, t_taken = borda_count(df,5)
    res_tuple = ky_method(df,voters_col=NUM_voters_col)
    ky_orders_lst = [int(pre.split(' ')[1]) for pre in res_tuple]
  # borda_dict = borda_count(df_fill,5)
    label = int(res_tuple[0].split(" ")[1])
    labels_lst.append(label)
    orders_lst.append(ky_orders_lst)

  return labels_lst, orders_lst

#do feature engineering to get one row from one chunck
def fe_process(df):

  return_df = pd.DataFrame()
  cols = list(df.columns)
  if NUM_voters_col in cols:
    cols.remove(NUM_voters_col)

  if "vote_percentage" in cols:
    cols.remove("vote_percentage")
  #dict_cand_avg = {}

  for col in cols:
    df_new = df[[NUM_voters_col,col]]
    df_new['avg_numerator'] = df_new[NUM_voters_col] * df_new[col]
    avg_num = np.sum(df_new['avg_numerator'])/np.sum(df_new[NUM_voters_col])

    #dict_cand_avg[col] = avg_num
    return_df[col] = [avg_num]

  return return_df

# Prepare final data frame which have target columns like labels for single winner and orders for muilti winner
def get_final_df(df_lst,labels_lst,orders_lst):
  i = 0
  for df in df_lst:

    avg_df = fe_process(df)
    #avg_df['target'] = [lab]

    if i==0:
      final_df = avg_df.copy()

    else:
      final_df = final_df.append(avg_df)

    i = i+1

  final_df['target'] = labels_lst
  final_df['orders'] = orders_lst
  final_df['orders'] = final_df['orders'].apply(lambda x:[int(num) for num in list(x) if num not in ["[","]",","," "]])
  if 'vote_percentag' in list(final_df.columns):
    final_df = final_df.drop(columns=['vote_percentage'])

  return final_df

#To call all Functions

df_preprocessed = preprocess_and_imputation(data_raw)
df_7cand_data = make_7_cand_data(df_preprocessed)
df_7cand_data = preference_to_candidate(df_7cand_data)
df_lst = generate_chunks(df_7cand_data,5)
labels_lst, orders_lst = gen_target_and_orders(df_lst)
df_final = get_final_df(df_lst,labels_lst,orders_lst)

df_7cand_data.isnull().sum()

#Rename the columns name in dataframe

rename_dict = {}
for col in df_final.columns:
  if col not in ['vote_percentage','target','orders']:
    new_col = "Candidate " + col.split(" ")[1]
    rename_dict[col] = new_col

df_final = df_final.rename(columns=rename_dict)
df_final.head()

# TO save final dataframe in csv file
# df_final.to_csv("/content/drive/MyDrive/kevel_proj_final_df.csv",index=False)

#Read csv file as input for machine learning model training and testing
import pandas as pd
df_final = pd.read_csv("/content/drive/MyDrive/kevel_proj_final_df.csv")

"""# **Order prediction (Multi Winner) - Model Training**"""

# train, test, split

# Import this package to split a dataset into training and testing subsets
from sklearn.model_selection import train_test_split

df_final['orders'] = df_final['orders'].apply(lambda x:[int(num) for num in list(x) if num not in ["[","]",","," "]])

x = df_final.drop(columns=['target'])
y = df_final['target'].values

# Split 20% dataset for testing part and 80%  for Testing
xtrain,xtest,ytrain_label,ytest_label = train_test_split(x,y,test_size=0.2,stratify=y,random_state=36)

ytrain_order = xtrain['orders'].values
ytest_order = xtest['orders'].values

xtrain = xtrain.drop(columns=['orders'])
xtest = xtest.drop(columns=['orders'])

# to get the predicted order by model
def get_predicted_orders(model_proba):   #model_proba = model.predict_proba(x)

  pred_odr_lst = []
  for prob_lst in model_proba:

    temp_lst = [val+1 for val in np.argsort(-prob_lst)]

    pred_odr_lst.append(temp_lst)

  return pred_odr_lst

# preparing accuracy lists to calculate final mean accuracy
def get_accuracy(true_orders,pred_orders):

  acc_lst, tru_lst, mis_lst = [], [], []
  for tru,pred in zip(true_orders,pred_orders):
    #print(type(tru), type(pred))

    #tru = list(tru)
    #print(type(tru[0]), type(pred[0]))
    sub_array = np.array(tru) - np.array(pred)

    mis_predicted = np.count_nonzero(sub_array)
    truly_predicted = NUM_candidates - mis_predicted
    acc = (truly_predicted/NUM_candidates)*100
    acc_lst.append(acc)

  final_accu = np.mean(acc_lst)
  return final_accu

#Import ML model which will used for training and testing
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from tabulate import tabulate
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier

# model training and experiments

def model_training_and_experiment(xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order):

  model_name, train_accuracy, test_accuracy = [],[],[]
  for key, model in {"Naive Bayes":MultinomialNB(), "Logistic regression":LogisticRegression(),
                     "Decision tree":DecisionTreeClassifier(), "Random forest":RandomForestClassifier(),
                     "SVM":SVC(kernel='rbf', probability=True),"Boosting_model":GradientBoostingClassifier()}.items():
    model.fit(xtrain,ytrain_label)

    # Predicted probabilities for the training and testing sets are obtained using the trained model.
    pred_proba_train = model.predict_proba(xtrain)
    pred_proba_test = model.predict_proba(xtest)

    # The predicted orders are generated from the predicted probabilities
    pred_train_orders = get_predicted_orders(pred_proba_train)
    pred_test_orders = get_predicted_orders(pred_proba_test)

    # get accuracy for trained and tested model
    model_acc_train = get_accuracy(ytrain_order,pred_train_orders)
    model_acc_test = get_accuracy(ytest_order,pred_test_orders)

    model_name.append(key)
    train_accuracy.append(model_acc_train)
    test_accuracy.append(model_acc_test)

  # Create accuracy table for all used models
  model_res_df = pd.DataFrame()
  model_res_df['Model'] = model_name
  model_res_df['Train_accuracy'] = train_accuracy
  model_res_df['Test_accuracy'] = test_accuracy

  print(tabulate(model_res_df, headers = 'keys', tablefmt = 'grid'))
  return

# call the function
model_training_and_experiment(xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order)

# hyperparameter tunning for Kemeny Young multi winner prediction
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid

def hp_tunning_for_order_prediction(final_model,xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order):

  model = final_model
  param_dict = {'penalty':['l1', 'l2', 'elasticnet'],
                'C':[0.1,0.2,0.5,2,3,4,5,6,7,8,9],
                "fit_intercept":[True, False],
                'class_weight':['balanced',None]}

  # creates a list of parameter combinations
  para_combo_lst = list(ParameterGrid(param_dict))
  train_accuracy, test_accuracy = [],[]
  model_res_df = pd.DataFrame(columns=["C",'penalty','fit_intercept','class_weight'])

  for combo in para_combo_lst:

    if combo['penalty'] == "elasticnet":

      model = model.set_params(penalty=combo['penalty'],C=combo['C'],fit_intercept=combo['fit_intercept'],class_weight=combo['class_weight'],solver='saga',l1_ratio=0.4)
    else:
      model = model.set_params(penalty=combo['penalty'],C=combo['C'],fit_intercept=combo['fit_intercept'],class_weight=combo['class_weight'],solver='saga')

    model.fit(xtrain,ytrain_label)
    pred_proba_train = model.predict_proba(xtrain)
    pred_proba_test = model.predict_proba(xtest)

    pred_train_orders = get_predicted_orders(pred_proba_train)
    pred_test_orders = get_predicted_orders(pred_proba_test)

    model_acc_train = get_accuracy(ytrain_order,pred_train_orders)
    model_acc_test = get_accuracy(ytest_order,pred_test_orders)

   # model_name.append(key)
    train_accuracy.append(model_acc_train)
    test_accuracy.append(model_acc_test)
    model_res_df = model_res_df.append(combo,ignore_index=True)

  model_res_df['Train_accuracy'] = train_accuracy
  model_res_df['Validation_accuracy'] = test_accuracy

  model_res_df = model_res_df.sort_values(by=['Validation_accuracy',"Train_accuracy"],ascending=[False,False]).reset_index(drop=True)

  print(tabulate(model_res_df, headers = 'keys', tablefmt = 'grid'))
  return model_res_df

# Call the function
hp_tunning_df_for_order = hp_tunning_for_order_prediction(LogisticRegression(),xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order)

#get final model with best parameter combination
def get_final_model(hp_tunning_df,xtrain,ytrain_label):

  if hp_tunning_df.loc[0,'penalty'] == "elasticnet":

    final_model = LogisticRegression(penalty=hp_tunning_df.loc[0,'penalty'],
                             C=hp_tunning_df.loc[0,'C'],
                             fit_intercept=hp_tunning_df.loc[0,'fit_intercept'],
                             class_weight=hp_tunning_df.loc[0,'class_weight'],
                             solver='saga',l1_ratio=0.4)

  else:
    final_model = LogisticRegression(penalty=hp_tunning_df.loc[0,'penalty'],
                              C=hp_tunning_df.loc[0,'C'],
                              fit_intercept=hp_tunning_df.loc[0,'fit_intercept'],
                              class_weight=hp_tunning_df.loc[0,'class_weight'],
                              solver='saga')

  final_model.fit(xtrain,ytrain_label)

  return final_model

# oreder prediction

def order_predictor(election_data):

  if election_data.isnull().sum().sum() != 0:
    print("Found missing values in election data, please cheak the data")
    return

  else:

    final_model = get_final_model(hp_tunning_df_for_order,xtrain,ytrain_label)

    df_after_fe = fe_process(election_data)
    model_proba = final_model.predict_proba(df_after_fe)
    election_result = get_predicted_orders(model_proba)

    ele_res_df = pd.DataFrame(columns=['Rank', "Candidate"])
    ele_res_df['Rank'] = [ele+1 for ele in range(NUM_candidates)]
    ele_res_df['Candidate'] = ["Candidate "+str(cand) for cand in election_result[0]]

    return ele_res_df

df_preprocessed = preprocess_and_imputation(data_raw)
df_7cand_data = make_7_cand_data(df_preprocessed)
df_7cand_data = preference_to_candidate(df_7cand_data)

col_replace = {}
for col in list(df_7cand_data.columns):
  if col != NUM_voters_col:
    new_col = col.replace("Preference","Candidate")
    col_replace[col] = new_col

df_7cand_data = df_7cand_data.rename(columns=col_replace)

#final result
import timeit
startTime = timeit.default_timer()
res = order_predictor(df_7cand_data)
print(res)
endTime = timeit.default_timer()
time_taken = endTime - startTime
print(f"Time Taken by ML model is: {time_taken}")

"""# **Single label prediction (only winner) -- Model training**



"""

# Import matrices to show result evolution
from sklearn.metrics import precision_score, accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def model_training_and_experiment_single_label(xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order):

  model_name, train_accuracy, test_accuracy, test_precision, train_precision = [],[],[],[],[]
  for key, model in {"Naive Bayes":MultinomialNB(), "Logistic regression":LogisticRegression(),
                     "Decision tree":DecisionTreeClassifier(), "Random forest":RandomForestClassifier(),
                     "SVM":SVC(kernel='rbf', probability=True)}.items():
    model.fit(xtrain,ytrain_label)

    # Predicted probabilities for the training and testing sets are obtained using the trained model.
    pred_label_train = model.predict(xtrain)
    pred_label_test = model.predict(xtest)

    #pred_train_orders = get_predicted_orders(pred_proba_train)
    #skpred_test_orders = get_predicted_orders(pred_proba_test)

    # get accuracy for trained and tested model
    model_acc_train = accuracy_score(ytrain_label,pred_label_train)
    model_acc_test = accuracy_score(ytest_label,pred_label_test)

    # get Presicion for trained and tested model
    model_pre_train = precision_score(ytrain_label,pred_label_train,average='macro')
    model_pre_test = precision_score(ytest_label,pred_label_test,average='macro')

    # get confusion of predicted and True label
    cm = confusion_matrix(ytest_label,pred_label_test)
    disp = ConfusionMatrixDisplay(cm)
   # plot_confusion_matrix(model, xtest, ytest_lable)

    disp.plot()
    plt.suptitle(f"Model: {key}")
    plt.show()

    model_name.append(key)
    train_accuracy.append(model_acc_train)
    test_accuracy.append(model_acc_test)
    train_precision.append(model_pre_train)
    test_precision.append(model_pre_test)

  # Create a models performance table
  model_res_df = pd.DataFrame()
  model_res_df['Model'] = model_name
  model_res_df['Train_accuracy'] = train_accuracy
  model_res_df['Test_accuracy'] = test_accuracy

  model_res_df['Train_precision'] = train_precision
  model_res_df['Test_precision'] = test_precision

  print(tabulate(model_res_df, headers = 'keys', tablefmt = 'grid'))
  return

model_training_and_experiment_single_label(xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order)

import collections
collections.Counter(ytest_label)

# hyperparameter tunning for single winner prediction

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid

def hp_tunning_for_label_prediction(final_model,xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order):

  model = final_model
  param_dict = {'penalty':['l1', 'l2', 'elasticnet'],
                'C':[0.1,0.2,0.5,2,3,4,5,6,7,8,9],
                "fit_intercept":[True, False],
                'class_weight':['balanced',None]}

  # give a list of parameter combinations
  para_combo_lst = list(ParameterGrid(param_dict))
  train_accuracy, test_accuracy = [],[]
  model_res_df = pd.DataFrame(columns=["C",'penalty','fit_intercept','class_weight'])

  for combo in para_combo_lst:

    if combo['penalty'] == "elasticnet":

      model = model.set_params(penalty=combo['penalty'],C=combo['C'],fit_intercept=combo['fit_intercept'],class_weight=combo['class_weight'],solver='saga',l1_ratio=0.4)
    else:
      model = model.set_params(penalty=combo['penalty'],C=combo['C'],fit_intercept=combo['fit_intercept'],class_weight=combo['class_weight'],solver='saga')

    model.fit(xtrain,ytrain_label)

    pred_label_train = model.predict(xtrain)
    pred_label_test = model.predict(xtest)

    model_pre_train = precision_score(ytrain_label,pred_label_train,average='macro')
    model_pre_test = precision_score(ytest_label,pred_label_test,average='macro')

   # model_name.append(key)
    train_accuracy.append(model_pre_train)
    test_accuracy.append(model_pre_test)
    model_res_df = model_res_df.append(combo,ignore_index=True)

  model_res_df['Train_precision'] = train_accuracy
  model_res_df['Validation_precision'] = test_accuracy

  model_res_df = model_res_df.sort_values(by=['Validation_precision',"Train_precision"],ascending=[False,False]).reset_index(drop=True)

  print(tabulate(model_res_df, headers = 'keys', tablefmt = 'grid'))
  return model_res_df

hp_tunning_df_for_label = hp_tunning_for_label_prediction(LogisticRegression(),xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order)

# predict the label result
def label_predictor(election_data):

  if election_data.isnull().sum().sum() != 0:
    print("Found missing values in election data, please cheak the data")
    return

  else:

    final_model = get_final_model(hp_tunning_df_for_label,xtrain,ytrain_label)

    df_after_fe = fe_process(election_data)
    election_result = final_model.predict(df_after_fe)

  return f"Winner of election is Candidate {election_result[0]}"

#final result
import timeit
startTime = timeit.default_timer()
final_result2 = label_predictor(df_7cand_data)
print(final_result2)
endTime = timeit.default_timer()
time_taken = endTime - startTime
print(f"Time Taken by ML model is: {time_taken}")

"""## Experiments Section"""

# intial experiments using borda counts

import timeit
def borda_count(df,num_candidates):                   # gives borda scores
 # startTime = timeit.default_timer()
  data = df.copy()
  cl = list(data.columns)
  cl.remove('Number of Voters')

  for cand in cl:
    data[cand+'score'] = data['Number of Voters'] * ((num_candidates+1) - data[cand])

  score_dict = {}

  for cand in cl:
    score_dict[cand] = np.sum(data[cand+'score'].values)
  rd = sorted(score_dict.items(),key=lambda x:x[1],reverse=True)
 # endTime = timeit.default_timer()
  #time_taken = endTime - startTime
  return rd

import timeit
startTime = timeit.default_timer()
endTime = timeit.default_timer()
time_taken = endTime - startTime

# data preparation for deep learning experiments

from keras.models import Model
from keras.layers import Dense, Dropout, Input
from keras.utils import to_categorical
import tensorflow as tf
import numpy as np

ytrain_label_dl = ytrain_label-1
ytest_label_dl = ytest_label-1
ytraindl = tf.keras.utils.to_categorical(ytrain_label_dl,num_classes=7)
ytestdl = tf.keras.utils.to_categorical(ytest_label_dl,num_classes=7)

ytrain_or_dl = np.array([lst for lst in ytrain_order])
ytest_or_dl = np.array([lst for lst in ytest_order])

# Deep Learning experiments

input = Input(shape=(7,))

l01 = Dense(units=500,activation='relu')(input)
do = Dropout(rate=0.5)(l01)
l02 = Dense(units=400)(do)
do1 = Dropout(rate=0.5)(l02)
l0 = Dense(units=200,activation='relu')(do1)
do2 = Dropout(rate=0.4)(l0)
l1 = Dense(units=100,activation='relu')(do2)
#l2 = Dense(units=2)(l01)
do2 = Dropout(rate=0.4)(l1)
l12 = Dense(units=50,activation='relu')(do2)
l3 = Dense(units=30,activation='relu')(l12)
fin = Dense(units=30)(l3)
out = Dense(units=7)(fin)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath="/content/best_dl_model.h5",
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

model = Model(inputs=[input],outputs=out)
model.compile(optimizer='adam',loss="mean_absolute_error",metrics=['accuracy'])
model.fit(xtrain,ytrain_or_dl,batch_size=8,epochs=1000
          ,validation_data=(xtest,ytest_or_dl),callbacks=[model_checkpoint_callback])

from keras.utils.vis_utils import plot_model

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# Deep Learning model evaluation

from keras.models import load_model
import scipy.stats as ss
model = load_model("/content/best_dl_model.h5")

#dl orders pred result
pred_ord_dl = model.predict(xtest)
def get_dl_orders(pred_ord_dl):
  result_lst = []
  for pred in pred_ord_dl:
    res = ss.rankdata(pred)
    result_lst.append(res)
  return result_lst

final_dl_ord = get_dl_orders(pred_ord_dl)

acc = get_accuracy(ytest_order,final_dl_ord)
print(f"Accuracy of deep learning model in predicting orders: {acc}")

"""# Conclusion and Observations"""

conclusion_df = pd.DataFrame()
conclusion_df['Method'] = ['Kemeny young method',"ML Model"]
conclusion_df['Execution_time'] = ["1 to 1.5 Minutes","0.05 to 0.1 Second"]
conclusion_df['Accuracy'] = ["-","Multi Winner = 60% || Single Winner = 86%"]
conclusion_df['Conclusion'] = ["ML model is 1000x faster than Kemeny young method", ""]
print(tabulate(conclusion_df, headers = 'keys', tablefmt = 'fancy_grid'))



"""# Building Predictor for any Number of candidates"""

#inputs

ele_data = pd.read_csv('/content/drive/MyDrive/soi.csv')
NUM_candidates = 5
NUM_voters_col = "Number of Voters"

####

import warnings
warnings.filterwarnings("ignore")


ele_data = ele_data.dropna()
ele_data = ele_data.astype('int')
ele_data = ele_data.reset_index(drop=True)
df_7cand_data_any = preference_to_candidate(ele_data)

df_lst_any = generate_chunks(df_7cand_data_any,5)

labels_lst_any, orders_lst_any = gen_target_and_orders(df_lst_any)

df_final = get_final_df(df_lst_any,labels_lst_any,orders_lst_any)


from sklearn.model_selection import train_test_split

x = df_final.drop(columns=['target'])
y = df_final['target'].values

xtrain,xtest,ytrain_label,ytest_label = train_test_split(x,y,test_size=0.2,stratify=y,random_state=36)

ytrain_order = xtrain['orders'].values
ytest_order = xtest['orders'].values

xtrain = xtrain.drop(columns=['orders'])
xtest = xtest.drop(columns=['orders'])

# model training and experiments

# hyperparameter tunning for multi winner prediction
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ParameterGrid

def hp_tunning_for_order_prediction(final_model,xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order):

  model = final_model
  param_dict = {'penalty':['l1', 'l2', 'elasticnet'],
                'C':[0.1,0.2,0.5,2,3,4,5,6,7,8,9],
                "fit_intercept":[True, False],
                'class_weight':['balanced',None]}

  para_combo_lst = list(ParameterGrid(param_dict))
  train_accuracy, test_accuracy = [],[]
  model_res_df = pd.DataFrame(columns=["C",'penalty','fit_intercept','class_weight'])

  for combo in para_combo_lst:

    if combo['penalty'] == "elasticnet":

      model = model.set_params(penalty=combo['penalty'],C=combo['C'],fit_intercept=combo['fit_intercept'],class_weight=combo['class_weight'],solver='saga',l1_ratio=0.4)
    else:
      model = model.set_params(penalty=combo['penalty'],C=combo['C'],fit_intercept=combo['fit_intercept'],class_weight=combo['class_weight'],solver='saga')

    model.fit(xtrain,ytrain_label)
    pred_proba_train = model.predict_proba(xtrain)
    pred_proba_test = model.predict_proba(xtest)

    pred_train_orders = get_predicted_orders(pred_proba_train)
    pred_test_orders = get_predicted_orders(pred_proba_test)

    model_acc_train = get_accuracy(ytrain_order,pred_train_orders)
    model_acc_test = get_accuracy(ytest_order,pred_test_orders)

   # model_name.append(key)
    train_accuracy.append(model_acc_train)
    test_accuracy.append(model_acc_test)
    model_res_df = model_res_df.append(combo,ignore_index=True)

  model_res_df['Train_accuracy'] = train_accuracy
  model_res_df['Validation_accuracy'] = test_accuracy

  model_res_df = model_res_df.sort_values(by=['Validation_accuracy',"Train_accuracy"],ascending=[False,False]).reset_index(drop=True)

  #print(tabulate(model_res_df, headers = 'keys', tablefmt = 'grid'))
  return model_res_df

hp_tunning_df_for_order_any = hp_tunning_for_order_prediction(LogisticRegression(),xtrain,xtest,ytrain_label,ytest_label,ytrain_order,ytest_order)


def order_predictor(election_data):

  if election_data.isnull().sum().sum() != 0:
    print("Found missing values in election data, please cheak the data")
    return

  else:

    final_model = get_final_model(hp_tunning_df_for_order_any,xtrain,ytrain_label)

    df_after_fe = fe_process(election_data)
    model_proba = final_model.predict_proba(df_after_fe)
    election_result = get_predicted_orders(model_proba)

    ele_res_df = pd.DataFrame(columns=['Rank', "Candidate"])
    ele_res_df['Rank'] = [ele+1 for ele in range(NUM_candidates)]
    ele_res_df['Candidate'] = ["Candidate "+str(cand) for cand in election_result[0]]

    return ele_res_df

#result

election_data = ele_data   #<<<-----------your election data here
order_predictor(election_data)